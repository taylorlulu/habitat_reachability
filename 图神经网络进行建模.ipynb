{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43aafac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7366d5",
   "metadata": {},
   "source": [
    "## 创建GCN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "038e6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你已经获得了三种物品的bbox\n",
    "# 对每种物品的bbox进行编号，分别为0、1、2\n",
    "# 例如，boxes[0] 存储的是容器的bbox，boxes[1] 存储的是抓取物品的bbox，boxes[2] 存储的是干扰物品的bbox\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263c470",
   "metadata": {},
   "source": [
    "## 创建GAT网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82fb8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GAT, self).__init__()\n",
    "        # 使用了 GATConv 作为图神经网络的层。\n",
    "        # GATConv 是 GAT 中的一个注意力机制层，可以对节点之间的关系进行建模，并通过注意力机制来学习不同节点之间的重要性\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=1)\n",
    "        self.conv2 = GATConv(hidden_dim, output_dim, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11b028",
   "metadata": {},
   "source": [
    "## 模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecf4f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [\n",
    "    torch.tensor([[0, 0, 1, 1], [1, 1, 2, 2]]),  # 容器的bbox，假设有两个容器\n",
    "    torch.tensor([[1.5, 1.5, 2.5, 2.5], [2, 2, 3, 3]]),  # 抓取物品的bbox，假设有两个抓取物品\n",
    "    torch.tensor([[0.5, 0.5, 1.5, 1.5], [2.5, 2.5, 3.5, 3.5]]),  # 干扰物品的bbox，假设有两个干扰物品\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a57d4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建节点特征，每个节点的特征是它的bbox\n",
    "node_features = torch.cat(boxes, dim=0)\n",
    "\n",
    "# 创建边，这里简单地假设所有节点之间都有边\n",
    "num_nodes = sum([bbox.size(0) for bbox in boxes])\n",
    "edge_index = torch.tensor([\n",
    "    [i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j\n",
    "], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# 构建图数据\n",
    "data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# 定义模型\n",
    "model = GCN(input_dim=node_features.size(-1), hidden_dim=64, output_dim=2)\n",
    "\n",
    "# 定义模型2\n",
    "model2 = GAT(input_dim=node_features.size(-1), hidden_dim=64, output_dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db41d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4909, -0.9470],\n",
       "        [-0.4909, -0.9470],\n",
       "        [-0.4909, -0.9470],\n",
       "        [-0.4909, -0.9470],\n",
       "        [-0.4909, -0.9470],\n",
       "        [-0.4909, -0.9470]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "775d598d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7933, -0.6021],\n",
       "        [-0.7933, -0.6021],\n",
       "        [-0.7598, -0.6306],\n",
       "        [-0.7933, -0.6021],\n",
       "        [-0.7523, -0.6373],\n",
       "        [-0.7558, -0.6342]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624fbee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[6, 4], edge_index=[2, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17d3e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 2.0000, 2.0000],\n",
       "        [1.5000, 1.5000, 2.5000, 2.5000],\n",
       "        [2.0000, 2.0000, 3.0000, 3.0000],\n",
       "        [0.5000, 0.5000, 1.5000, 1.5000],\n",
       "        [2.5000, 2.5000, 3.5000, 3.5000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2558f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "         4, 5, 5, 5, 5, 5],\n",
       "        [1, 2, 3, 4, 5, 0, 2, 3, 4, 5, 0, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 1, 2, 3,\n",
       "         5, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b181cc",
   "metadata": {},
   "source": [
    "## 图神经网络进行建模的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da8c83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "class SceneGraph():\n",
    "    def __init__(self, base_tf, theta, dist2goal, obj_bboxes, obj_bboxes_indexes):\n",
    "        super(SceneGraph, self).__init__()\n",
    "        self.graph = None\n",
    "        self.data = None\n",
    "        self.robot_visible = False\n",
    "        self._device = 'cpu'\n",
    "        # 包含不同类型关系的列表，其中每个元素表示一个关系类型\n",
    "        # 这样做的目的是为了使得图在后续的处理中能够区分不同类型的边，即对应于边的权重\n",
    "        # 通过使用这些关系类型，代码可以正确地标识和区分从不同实体到机器人的边，从而构建了一个更加复杂的社交图\n",
    "        # 'o2r'：表示抓取物品旁的其他物品到机器人的关系，obstacle2robot，类似于circle obstacle\n",
    "        # 'c2r'：容器本身作为障碍物，container2robot，类似于line obstacle\n",
    "        # 's2r'：容器附近的障碍物，surrounding2robot\n",
    "        self.rels = ['o2r', 'c2r', 's2r']\n",
    "        self.mode = 0\n",
    "\n",
    "        # ToDo:第一步，首先要将坐标转换到世界坐标系下\n",
    "        rotated_bboxes = self.rotate_state(base_tf, theta, obj_bboxes)\n",
    "\n",
    "        # ToDo:第二步，再转换为图的数据结构\n",
    "        self.build_up_graph_on_local_state(base_tf, theta, dist2goal, rotated_bboxes, obj_bboxes_indexes)\n",
    "\n",
    "    def rotate_state(self, base_tf, theta, obj_bboxes):\n",
    "        \"\"\"\n",
    "        将其他物品的状态转到机器人坐标系下\n",
    "        Args:\n",
    "            base_tf:当前robot的坐标系\n",
    "            theta:当前robot的朝向\n",
    "            obj_bboxes:obj_bboxes的维度为[n, 5]其中0:4为bbox，4为高度z，5为朝向\n",
    "            obj_bboxes_indexes:\n",
    "\n",
    "        Returns:\n",
    "        Transform the coordinate to agent-centric.\n",
    "        Input tuple include robot state tensor and human state tensor.\n",
    "        robot state tensor is of size (number, state_length)(for example 1*9)\n",
    "        obstacle state tensor is of size (number, state_length)(for example 3*4)\n",
    "        container state tensor is of size (number, state_length)(for example 5*4)\n",
    "        surrounding state tensor is of size (number, state_length)(for example 4*4)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"第一步：将物体转移到机器人坐标系下\"\"\"\n",
    "        inv_base_tf = torch.linalg.inv(base_tf)\n",
    "        \n",
    "        \"\"\"第二步：分别获取障碍物坐标的索引值\"\"\"\n",
    "        local_obj_bboxes = torch.zeros_like(obj_bboxes)\n",
    "\n",
    "        total_num = len(obj_bboxes)\n",
    "\n",
    "        for obj_num in range(total_num):\n",
    "            # 获取当前对象轴对齐边界框的最小的 XY 顶点坐标\n",
    "            min_xy_vertex = torch.hstack(\n",
    "                (obj_bboxes[obj_num, 0:2], torch.tensor([0.0, 1.0], device=self._device))).T\n",
    "            # 获取当前对象轴对齐边界框的最大的 XY 顶点坐标\n",
    "            max_xy_vertex = torch.hstack(\n",
    "                (obj_bboxes[obj_num, 2:4], torch.tensor([0.0, 1.0], device=self._device))).T\n",
    "            \n",
    "            # 通过矩阵乘法将最小顶点坐标转换到机器人参考框架中，并更新为新的最小顶点坐标\n",
    "            new_min_xy_vertex = torch.matmul(inv_base_tf, min_xy_vertex)[0:2].T.squeeze()\n",
    "            # 通过矩阵乘法将最大顶点坐标转换到机器人参考框架中，并更新为新的最大顶点坐标\n",
    "            new_max_xy_vertex = torch.matmul(inv_base_tf, max_xy_vertex)[0:2].T.squeeze()\n",
    "            \n",
    "            # 记录结果\n",
    "            local_obj_bboxes[obj_num, 0:4] = torch.hstack((new_min_xy_vertex, new_max_xy_vertex))\n",
    "            # 设置高度差\n",
    "            local_obj_bboxes[obj_num, 5] = self.limit_angle(obj_bboxes[obj_num, 5] - theta)\n",
    "\n",
    "        return local_obj_bboxes\n",
    "\n",
    "\n",
    "    def limit_angle(self, angle):\n",
    "        # 将角度限制在 -π 到 π 之间\n",
    "        while angle < -np.pi:\n",
    "            angle += 2 * np.pi\n",
    "        while angle > np.pi:\n",
    "            angle -= 2 * np.pi\n",
    "        return angle\n",
    "\n",
    "\n",
    "    def build_up_graph_on_local_state(self, base_tf, theta, dist2goal, obj_bboxes, obj_bboxes_indexes):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            base_tf: 机器人当前的坐标系\n",
    "            theta: 机器人当前的朝向\n",
    "            obj_bboxes: 物品的bboxes\n",
    "            obj_bboxes_indexes: 物品的索引值\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        src_id = torch.Tensor([])\n",
    "        dst_id = torch.Tensor([])\n",
    "        # We create a map to store the types of the nodes. We'll use it to compute edges' types\n",
    "        self.typeMap = dict()\n",
    "        position_by_id = {}\n",
    "\n",
    "        # Node Descriptor Table\n",
    "        # 建立四种类型的节点，r表示robot，h表示human，o表示obstacle，w表示wall\n",
    "        self.node_descriptor_header = ['r', 'o', 'c', 's']\n",
    "\n",
    "        # Relations are integers\n",
    "        node_types_one_hot = ['robot', 'obstacle', 'container', 'surrounding']\n",
    "\n",
    "        # 机器人矩阵的特征\n",
    "        robot_metric_features = ['rob_x', 'rob_y', 'rob_ori', 'dis2goal']\n",
    "\n",
    "        # 桌面上其他障碍物矩阵的特征\n",
    "        obstacle_metric_features = ['obs_min_x', 'obs_min_y', 'obs_max_x', 'obs_max_y', 'obs_z', 'obs_ori']\n",
    "\n",
    "        # 障碍物矩阵的特征\n",
    "        container_metric_features = ['con_min_x', 'con_min_y', 'con_max_x', 'con_max_y', 'con_z', 'con_ori']\n",
    "\n",
    "        # 墙矩阵的特征\n",
    "        surrounding_features = ['surr_min_x', 'surr_min_y', 'surr_max_x', 'surr_max_y', 'surr_z', 'surr_ori']\n",
    "\n",
    "        # 所有的特征为特征之和\n",
    "        all_features = node_types_one_hot + robot_metric_features + obstacle_metric_features + container_metric_features + surrounding_features\n",
    "\n",
    "        # Copy input data\n",
    "        # 复制输入数据\n",
    "        self.obj_bboxes = obj_bboxes\n",
    "\n",
    "        # 输入数据分别表示的含义\n",
    "        robot_state = np.hstack([base_tf[:3, 3], theta, dist2goal])\n",
    "        obstacle_state = self.obj_bboxes[obj_bboxes_indexes[0], :]\n",
    "        container_state = self.obj_bboxes[obj_bboxes_indexes[1], :]\n",
    "        surrounding_state = self.obj_bboxes[obj_bboxes_indexes[2], :]\n",
    "\n",
    "        # 特征的维度\n",
    "        feature_dimensions = len(all_features)\n",
    "\n",
    "        # 统计各个物品的数目\n",
    "        # 机器人的数目\n",
    "        robot_num = 1\n",
    "\n",
    "        # 障碍物的数目\n",
    "        if obstacle_state is not None:\n",
    "            obstacle_num = obstacle_state.shape[0]\n",
    "        else:\n",
    "            obstacle_num = 0\n",
    "\n",
    "        # 容器的数目，这是一个广义的容器，因为对于桌子只有一个长方体，对于沙发有三个长方体，对于冰箱有四个长方体\n",
    "        if container_state is not None:\n",
    "            container_num = container_state.shape[0]\n",
    "        else:\n",
    "            container_num = 0\n",
    "\n",
    "        # 容器周围障碍物的数目\n",
    "        if surrounding_state is not None:\n",
    "            surrounding_num = surrounding_state.shape[0]\n",
    "        else:\n",
    "            surrounding_num = 0\n",
    "\n",
    "        # 统计总的节点的数目\n",
    "        total_node_num = robot_num + obstacle_num + container_num + surrounding_num\n",
    "\n",
    "        # robot的tensor\n",
    "        robot_tensor = torch.zeros((robot_num, feature_dimensions))\n",
    "\n",
    "        # 记录机器人的特征\n",
    "        robot_tensor[0, all_features.index('robot')] = 1\n",
    "        # 记录机器人的state特征\n",
    "        robot_tensor[0, all_features.index('rob_x'):all_features.index(\"rob_ori\") + 1] = robot_state[0]\n",
    "        # 将features记录为robot_tensor\n",
    "        features = robot_tensor\n",
    "\n",
    "        # 记录人类特征\n",
    "        if obstacle_num > 0:\n",
    "            obstacle_tensor = torch.zeros((obstacle_num, feature_dimensions))\n",
    "            for i in range(obstacle_num):\n",
    "                obstacle_tensor[i, all_features.index('obstacle')] = 1\n",
    "                obstacle_tensor[i, all_features.index('obs_min_x'):all_features.index(\"obs_ori\") + 1] = \\\n",
    "                obstacle_state[i]\n",
    "\n",
    "            # self.graph.nodes['human'].data['h'] = human_tensor\n",
    "            features = torch.cat([features, obstacle_tensor], dim=0)\n",
    "\n",
    "        # 记录障碍物特征\n",
    "        if container_num > 0:\n",
    "            container_tensor = torch.zeros((container_num, feature_dimensions))\n",
    "            for i in range(container_num):\n",
    "                container_tensor[i, all_features.index('container')] = 1\n",
    "                container_tensor[i, all_features.index('con_min_x'):all_features.index(\"con_ori\") + 1] = \\\n",
    "                    container_state[i]\n",
    "            # self.graph.nodes['obstacle'].data['h'] = obstacle_tensor\n",
    "            features = torch.cat([features, container_tensor], dim=0)\n",
    "\n",
    "        # 记录墙特征\n",
    "        if surrounding_num > 0:\n",
    "            surrounding_tensor = torch.zeros((surrounding_num, feature_dimensions))\n",
    "            for i in range(surrounding_num):\n",
    "                surrounding_tensor = torch.zeros((surrounding_num, feature_dimensions))\n",
    "                surrounding_tensor[i, all_features.index('surrounding')] = 1\n",
    "                surrounding_tensor[i, all_features.index('surr_min_x'):all_features.index(\"surr_ori\") + 1] = \\\n",
    "                    surrounding_state[i]\n",
    "            features = torch.cat([features, surrounding_tensor], dim=0)\n",
    "        # self.graph.nodes['wall'].data['h'] = wall_tensor\n",
    "        # features = torch.cat([robot_tensor, human_tensor, obstacle_tensor, wall_tensor], dim=0)\n",
    "\n",
    "        ### build up edges for the social graph\n",
    "        # add obstacle_to_robot edges\n",
    "        # 创建了一些空的张量来存储边的信息\n",
    "        src_id = torch.Tensor([])  # 源节点 ID\n",
    "        dst_id = torch.Tensor([])  # 目标节点 ID (dst_id)\n",
    "        edge_types = torch.Tensor([])  # 边的类型 (edge_types)\n",
    "        edge_norm = torch.Tensor([])  # 边的归一化值 (edge_norm)\n",
    "        # add human_to_robot edges\n",
    "\n",
    "        # 如果存在障碍物 (obstacle_num > 0)，则创建从障碍物到机器人的边\n",
    "        if obstacle_num > 0:\n",
    "            # 生成障碍物的源节点 ID (src_obstacle_id)\n",
    "            # 这部分相当于是对起始点索引的编号\n",
    "            src_obstacle_id = torch.tensor(range(obstacle_num)) + robot_num\n",
    "            # 将目标节点 ID (o2r_robot_id) 设置为零向量，表示所有这些边都指向机器人\n",
    "            # 终止点索引\n",
    "            o2r_robot_id = torch.zeros_like(src_obstacle_id)\n",
    "            # 这行代码为边的类型创建了一个张量，这部分是乘以边的权重\n",
    "            o2r_edge_types = torch.ones_like(o2r_robot_id) * torch.LongTensor([self.rels.index('o2r')])\n",
    "            # 为边的归一化值创建了一个张量，将其设置为所有边的归一化值都为 1.0，无权图\n",
    "            o2r_edge_norm = torch.ones_like(o2r_robot_id) * (1.0)\n",
    "            src_id = src_obstacle_id\n",
    "            dst_id = o2r_robot_id\n",
    "            edge_types = o2r_edge_types\n",
    "            edge_norm = o2r_edge_norm\n",
    "\n",
    "        # 如果存在人物 (human_num > 0)，则创建从人物到机器人的边\n",
    "        if container_num > 0:\n",
    "            # 这行代码生成人物的源节点 ID，并将其与机器人的数量相加，以确保人物的 ID 与机器人的 ID 不重叠\n",
    "            src_human_id = torch.tensor(range(container_num)) + robot_num + obstacle_num\n",
    "            # 创建了目标节点ID，将其设置为与人物相同长度的零向量。这意味着所有的边都指向机器人\n",
    "            h2r_robot_id = torch.zeros_like(src_human_id)\n",
    "            # 为边的类型创建了一个张量，与之前类似，找到了关系列表中“人物到机器人”的索引，并创建了一个与目标节点 ID 相同长度的张量，并将其填充为相应的边类型\n",
    "            h2r_edge_types = torch.ones_like(h2r_robot_id) * torch.LongTensor([self.rels.index('c2r')])\n",
    "            # 为边的归一化值创建了一个张量，将其设置为所有边的归一化值都为 1.0，无权图\n",
    "            h2r_edge_norm = torch.ones_like(h2r_robot_id) * (1.0)\n",
    "            # 记录所有的边节点\n",
    "            src_id = torch.cat([src_id, src_human_id], dim=0)\n",
    "            dst_id = torch.cat([dst_id, h2r_robot_id], dim=0)\n",
    "            edge_types = torch.cat([edge_types, h2r_edge_types], dim=0)\n",
    "            edge_norm = torch.cat([edge_norm, h2r_edge_norm], dim=0)\n",
    "\n",
    "        # add wall_to_robot edges\n",
    "        # 如果存在墙壁，则创建从墙壁到机器人的边\n",
    "        if surrounding_num > 0:\n",
    "            # 这一行创建了墙壁节点的源节点 ID，相加，以确保源节点 ID 的唯一性\n",
    "            src_wall_id = torch.tensor(range(surrounding_num)) + robot_num + obstacle_num + container_num\n",
    "            # 创建了一个与墙壁数量相同的零向量，表示所有这些边都指向机器人\n",
    "            w2r_robot_id = torch.zeros_like(src_wall_id)\n",
    "            # 创建了墙壁的边权\n",
    "            w2r_edge_types = torch.ones_like(w2r_robot_id) * torch.LongTensor([self.rels.index('s2r')])\n",
    "            # 创建了归一化后的边权\n",
    "            w2r_edge_norm = torch.ones_like(w2r_robot_id) * (1.0)\n",
    "\n",
    "            src_id = torch.cat([src_id, src_wall_id], dim=0)\n",
    "            dst_id = torch.cat([dst_id, w2r_robot_id], dim=0)\n",
    "            edge_types = torch.cat([edge_types, w2r_edge_types], dim=0)\n",
    "            edge_norm = torch.cat([edge_norm, w2r_edge_norm], dim=0)\n",
    "\n",
    "        edge_norm = edge_norm.unsqueeze(dim=1)\n",
    "        edge_norm = edge_norm.float()\n",
    "        edge_types = edge_types.float()\n",
    "\n",
    "        # 通过dgl库创建图\n",
    "        self.graph = dgl.graph((src_id, dst_id), num_nodes=total_node_num, idtype=torch.int64)\n",
    "        self.graph.ndata['h'] = features\n",
    "        self.graph.edata.update({'rel_type': edge_types, 'norm': edge_norm})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f81ef",
   "metadata": {},
   "source": [
    "## 进行数据的填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "13cfd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_tf, theta, dist2goal, obj_bboxes, obj_bboxes_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a3874e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_xy = [1.0, 2.0]\n",
    "base_ori = 0.5\n",
    "\n",
    "base_tf = torch.tensor([[np.cos(base_ori), -np.sin(base_ori), 0, base_xy[0]],\n",
    "                   [np.sin(base_ori),  np.cos(base_ori),  0, base_xy[1]],\n",
    "                   [               0,                 0,  1,        1.2],\n",
    "                   [               0,                 0,  0,          1]], device='cpu', dtype=torch.double)\n",
    "\n",
    "dist2goal = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "64046395",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_bboxes = torch.rand(6, 6, dtype=torch.double)\n",
    "\n",
    "obj_bboxes_indexes = [[0, 1, 2], [3, 4], [5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88006dfd",
   "metadata": {},
   "source": [
    "## 运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f42b69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_graph = SceneGraph(base_tf, base_ori, dist2goal, obj_bboxes, obj_bboxes_indexes).graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "01728d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=7, num_edges=6,\n",
       "      ndata_schemes={'h': Scheme(shape=(26,), dtype=torch.float32)}\n",
       "      edata_schemes={'rel_type': Scheme(shape=(), dtype=torch.float32), 'norm': Scheme(shape=(1,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936272a0",
   "metadata": {},
   "source": [
    "## 进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "711f035f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'learned_robot_placement'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2674047/4177306221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlearned_robot_placement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'learned_robot_placement'"
     ]
    }
   ],
   "source": [
    "from learned_robot_placement.models.gnn_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ad747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HGAT",
   "language": "python",
   "name": "hrl4in"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
